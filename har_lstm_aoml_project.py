# -*- coding: utf-8 -*-
"""HAR_LSTM_AOML_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-XIyKLE1mStwFbiK-Jc2TK5fRS5-afl5
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from scipy.cluster import hierarchy

from google.colab import files
data=files.upload()

"""# Combining both Data sets as it would be better for EDA and cross validation"""

train_df=pd.read_csv('train.csv')
test_df=pd.read_csv('testml.csv')

print('Shape of Train Data :',train_df.shape)
print('Shape of Test Data :',test_df.shape)

"""**Checking the database for null values:**"""

print('Train Null Values:',train_df.isnull().values.sum())
print('Test Null Values:',test_df.isnull().values.sum())

"""The database does not include any missing values. Now we can move on to the next step, checking for duplicates in the data:

**Checking the database for duplicates:**
"""

print('Number of duplicates in train set:{}'.format(sum(train_df.duplicated())))
print('Number of duplicates in test set:{}'.format(sum(test_df.duplicated())))
#print('Duplicated data',train_df.duplicated().values.sum())

"""Checking if the train df and test df have the same columns"""

if train_df.columns.all() == test_df.columns.all():
    print('All columns are same')

df = pd.concat([train_df, test_df], ignore_index = True)

df.head()

data

print('Total Data Shape:',df.shape)

df['subject'].groupby(df['subject']).count()

"""The Database has 30 subjects and each indivual has performed around >300 tasks to make the total reach 10299"""

train_df['subject'].groupby(train_df['Activity']).value_counts()

"""So we have 30 subjects and each perfomed different trials.
Now lets look at how many trials of each activity exist in our train dataset:
"""

train_df['Activity'].groupby(train_df['Activity']).count()

"""# Conclusions till now
* We have 30 subjects.
* In database each datapoint is one of the six activities.
* Each subject repeated the activity several times
* Each Subject seems to have around equal contribution to dataset in tota but the contribution in each activity vary
* Since it is a classification problem, we can use multiple different models to train the dataset on

**Check for data imbalance:**
"""

px.pie(df,names='Activity',title='Activity in database')

"""So pie plot shows that the data is fairly balanced."""

px.histogram(data_frame=df,x='subject',color='Activity',barmode='group',title='Histogram of data')

"""The Data Seems quite consistent and balanced apart from some exceptions, to emphasize that lets make more plots"""

activity_counts = df.groupby(['subject', 'Activity']).size().reset_index(name='counts')
figures = []
for activity in df['Activity'].unique():
    activity_df = activity_counts[activity_counts['Activity'] == activity]
    fig = px.line(activity_df, x='subject', y='counts', title=f'Activity Counts for {activity}')
    figures.append(fig)
for figure in figures:
    figure.show()

activity = df['Activity'].unique()
figures = []
for i in activity:
    subject_id = np.random.randint(1,30)
    filtered_df = df[(df['subject'] == subject_id) & (df['Activity'] == i)]
    filtered_df = filtered_df.reset_index()
    fig = px.line(filtered_df, x='index', y='tBodyAcc-mean()-X', title=f'Time Segment Plot for Subject {subject_id} - {i}')
    figures.append(fig)
for fig in figures:
    fig.show()

"""This figure shows the time segment plots for each activity for random subjects, the gap between continuous values is due to Sampling and Noise Filtering. But these sampling rates are sufficient to capture variability in data

# Data exploration
"""

corrmat=df.drop(['Activity'],axis=1).corr()
f,ax=plt.subplots(figsize=(10,10))
sns.heatmap(corrmat,vmax=0.8,square=True, center = 0)
plt.show()

"""The dataset has lost of feaures that are highly correlated with each other.

This could be because of redudancy of data and derived features as the the 561 columns in the data are made by different statistical operations on the same sensor values

The almost balck(Dull columns) indicate no correlation
"""

linkage = hierarchy.linkage(hierarchy.distance.pdist(corrmat), method='ward')
order = hierarchy.leaves_list(hierarchy.optimal_leaf_ordering(linkage, hierarchy.distance.pdist(corrmat)))
corrmat_clustered = corrmat.iloc[order, order]
f, ax = plt.subplots(figsize=(15, 15))
sns.heatmap(corrmat_clustered, vmax=.8, center=0,
            square=True)
plt.show()

"""Blocks of High Correlation: The clustering has grouped features that are highly correlated (bright red areas), creating visible blocks. These blocks indicate groups of features that have similar relationships with others, possibly representing similar underlying factors.

Negative Correlations: There are also areas with strong negative correlations (bright blue), although less extensive than the positive ones. These features move inversely to each other and could be considered for feature engineering to capture inverse relationship

Therefore clustering the similar data in order is helpful as it made the correlation matrix more interpretables.
"""

px.box(train_df, x='Activity',y='tBodyAccMag-mean()')

"""Boxplot of mean magnitude of acceleration shows that dynamic activities are  much differnet from static activities"""

sensor_values = [
    'tBodyAcc-mean()-X',
    'tBodyAcc-mean()-Y',
    'tBodyAcc-mean()-Z',
]

filter  = df[df['Activity'].isin(['WALKING','WALKING_DOWNSTAIRS','WALKING_UPSTAIRS'  ])]
for sensor in sensor_values:
    fig = px.box(filter, x='Activity', y=sensor, title=f'Boxplot of {sensor} across different activities')
    fig.show()

"""We know that our featur space is huge and probably its better to find a manifold that is in lower dimention and easier to seperate groups from each other:
Lets start with PCA:
"""

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

X_train=train_df.drop('Activity',axis=1)
Y_train=train_df['Activity']
X_test=test_df.drop('Activity',axis=1)
Y_test=test_df['Activity']

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

label_encoder = LabelEncoder()
Y_train_encoded = label_encoder.fit_transform(Y_train)
Y_test_encoded = label_encoder.transform(Y_test)

"""# LSTM"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

def build_lstm_model(input_shape, output_shape):
    model = Sequential()
    model.add(LSTM(128, input_shape=input_shape))
    model.add(Dropout(0.5))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(output_shape, activation='softmax'))
    return model

def train_lstm_model(X_train, Y_train, X_val, Y_val, batch_size=32, epochs=50):
    model = build_lstm_model((X_train.shape[1], X_train.shape[2]), len(np.unique(Y_train)))
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    early_stopping = EarlyStopping(patience=5, restore_best_weights=True)
    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,
                        validation_data=(X_val, Y_val), callbacks=[early_stopping], verbose=1)
    return model, history

X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Train LSTM model
lstm_model, lstm_history = train_lstm_model(X_train_reshaped, Y_train_encoded,
                                            X_test_reshaped, Y_test_encoded)

# Plot training and validation curves
plt.figure(figsize=(12, 6))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(lstm_history.history['accuracy'])
plt.plot(lstm_history.history['val_accuracy'])
plt.title('Model accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(lstm_history.history['loss'])
plt.plot(lstm_history.history['val_loss'])
plt.title('Model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

"""# Confusion Matrix"""

def evaluate_model(model, X_test, Y_test):
    Y_pred_prob = model.predict(X_test)
    Y_pred = np.argmax(Y_pred_prob, axis=1)
    testing_accuracy = accuracy_score(Y_test, Y_pred)
    print("Testing Accuracy:", testing_accuracy)
    conf_matrix = confusion_matrix(Y_test, Y_pred)
    #print("Confusion Matrix:")
    #print(conf_matrix)

    # Normalize confusion matrix
    conf_matrix_norm = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]

    # Display confusion matrix as heatmap with percentages
    plt.figure(figsize=(10, 8))
    sns.heatmap(conf_matrix_norm, annot=True, fmt='.2%', cmap='Blues',
                xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
    plt.xlabel('Predicted labels')
    plt.ylabel('True labels')
    plt.title('Normalized Confusion Matrix')
    plt.show()

    return testing_accuracy, conf_matrix

# Evaluate LSTM model
testing_accuracy, conf_matrix = evaluate_model(lstm_model, X_test_reshaped, Y_test_encoded)

